# Provider Response Structure Analysis
**Fecha:** 4 de diciembre de 2025  
**PropÃ³sito:** Analizar quÃ© datos devuelve cada provider (raw) y cÃ³mo los procesamos

---

## ðŸŽ¯ Problema Actual

**Estamos perdiendo datos** porque:
1. Solo guardamos datos procesados (nuestro array normalizado)
2. No guardamos la respuesta RAW completa del provider
3. Diferentes providers devuelven estructuras distintas
4. No sabemos quÃ© metadatos Ãºnicos ofrece cada provider

---

## ðŸ“Š AnÃ¡lisis por Provider

### 1. **OpenAI** (via OpenAI PHP SDK)

#### Non-Streaming Response
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-4-turbo",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    "logprobs": null,
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 56,
    "completion_tokens": 31,
    "total_tokens": 87,
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  }
}
```

#### Streaming Response (Ãºltimo chunk)
```php
// SDK devuelve objeto CreateStreamedResponse
$lastResponse->id                    // "chatcmpl-123"
$lastResponse->model                 // "gpt-4-turbo"
$lastResponse->created               // 1677652288
$lastResponse->systemFingerprint     // "fp_44709d6fcb"
$lastResponse->usage->promptTokens   // 56 âœ…
$lastResponse->usage->completionTokens // 31 âœ…
$lastResponse->usage->totalTokens    // 87 âœ…
$lastResponse->choices[0]->finishReason // "stop"
```

**Metadatos Ãºnicos OpenAI:**
- âœ… `system_fingerprint` - Huella del sistema
- âœ… `completion_tokens_details` - Tokens de reasoning, predictions
- âœ… `created` - Timestamp Unix

---

### 2. **OpenRouter** (via OpenAI PHP SDK)

#### Non-Streaming Response
```json
{
  "id": "gen-1764824848-rKblVd0t2QQTXTDbMQNe",
  "model": "anthropic/claude-sonnet-4.5",
  "object": "chat.completion",
  "created": 1764824848,
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 56,
    "completion_tokens": 31,
    "total_tokens": 87,
    "native_tokens_prompt": 58,        // â­ OPENROUTER-SPECIFIC
    "native_tokens_completion": 29     // â­ OPENROUTER-SPECIFIC
  }
}
```

#### Streaming Response (Ãºltimo chunk) - **PROBLEMA**
```php
// SDK devuelve objeto CreateStreamedResponse
$lastResponse->id                    // "gen-..." âœ…
$lastResponse->model                 // "anthropic/claude-sonnet-4.5" âœ…
$lastResponse->created               // 1764824848 âœ…
$lastResponse->systemFingerprint     // null (no usa este campo)
$lastResponse->usage                 // âŒ NULL - AQUÃ ESTÃ EL PROBLEMA
$lastResponse->choices[0]->finishReason // "stop" âœ…
```

**Metadatos Ãºnicos OpenRouter:**
- â­ `native_tokens_prompt` - Tokens reales del modelo subyacente (ej: Claude usa mÃ¡s tokens)
- â­ `native_tokens_completion` - Tokens reales de respuesta del modelo
- â­ `id` formato: `gen-{timestamp}-{hash}` (diferente a OpenAI)
- âŒ NO envÃ­a `usage` en Ãºltimo chunk de stream (DIFERENCIA CLAVE)

**Headers HTTP adicionales (no capturamos actualmente):**
```
X-RateLimit-Requests-Limit: 200
X-RateLimit-Requests-Remaining: 199
X-RateLimit-Tokens-Limit: 10000000
X-RateLimit-Tokens-Remaining: 9999943
```

---

### 3. **Anthropic** (via HTTP directo)

#### Non-Streaming Response
```json
{
  "id": "msg_01XYZ123abc",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "Hello! How can I help you?"
    }
  ],
  "model": "claude-3-opus-20240229",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {
    "input_tokens": 156,
    "output_tokens": 89
  }
}
```

#### Streaming Response Events
```
event: message_start
data: {"type":"message_start","message":{"id":"msg_01XYZ","usage":{"input_tokens":156}}}

event: content_block_delta
data: {"type":"content_block_delta","delta":{"text":"Hello"}}

event: message_delta
data: {"type":"message_delta","usage":{"output_tokens":89},"delta":{"stop_reason":"end_turn"}}

event: message_stop
data: {"type":"message_stop"}
```

**Metadatos Ãºnicos Anthropic:**
- âœ… `id` formato: `msg_01XYZ...`
- âœ… `stop_reason`: end_turn, max_tokens, stop_sequence
- âœ… `stop_sequence`: String que causÃ³ el stop (si aplica)
- âŒ NO tiene `system_fingerprint`
- âŒ NO tiene `created` timestamp
- âœ… Streaming por eventos SSE (diferente estructura)

---

### 4. **Ollama** (via HTTP directo)

#### Non-Streaming Response
```json
{
  "model": "deepseek-coder:6.7b",
  "created_at": "2025-12-04T04:06:54.123Z",
  "response": "Hello! How can I help you?",
  "done": true,
  "context": [1234, 5678, ...],  // Array de tokens
  "total_duration": 3448000000,
  "load_duration": 123000000,
  "prompt_eval_count": 156,
  "prompt_eval_duration": 810000000,
  "eval_count": 89,
  "eval_duration": 1200000000
}
```

#### Streaming Response (chunks)
```json
// Chunks intermedios
{"model":"deepseek-coder:6.7b","created_at":"...","response":"Hello","done":false}
{"model":"deepseek-coder:6.7b","created_at":"...","response":"!","done":false}

// Ãšltimo chunk
{
  "model": "deepseek-coder:6.7b",
  "created_at": "2025-12-04T04:06:54.123Z",
  "response": "",
  "done": true,
  "context": [1234, 5678, ...],
  "total_duration": 3448000000,
  "load_duration": 123000000,
  "prompt_eval_count": 156,
  "prompt_eval_duration": 810000000,
  "eval_count": 89,
  "eval_duration": 1200000000
}
```

**Metadatos Ãºnicos Ollama:**
- â­ `context` - Array de tokens completo (para continuar conversaciÃ³n)
- â­ `total_duration` - Nanosegundos totales
- â­ `load_duration` - Tiempo cargando modelo
- â­ `prompt_eval_duration` - Tiempo procesando prompt
- â­ `eval_duration` - Tiempo generando respuesta
- âœ… `done` flag - Indica Ãºltimo chunk
- âœ… Timestamps en formato ISO 8601

---

## ðŸ” Comparativa de Campos

| Campo | OpenAI | OpenRouter | Anthropic | Ollama |
|-------|--------|------------|-----------|--------|
| **ID Ãºnico** | âœ… chatcmpl-* | âœ… gen-* | âœ… msg_* | âŒ |
| **Timestamp** | âœ… created | âœ… created | âŒ | âœ… created_at |
| **Model usado** | âœ… | âœ… | âœ… | âœ… |
| **Tokens input** | âœ… prompt_tokens | âœ… prompt_tokens | âœ… input_tokens | âœ… prompt_eval_count |
| **Tokens output** | âœ… completion_tokens | âœ… completion_tokens | âœ… output_tokens | âœ… eval_count |
| **Tokens totales** | âœ… total_tokens | âœ… total_tokens | âŒ (calc) | âŒ (calc) |
| **Finish reason** | âœ… stop/length/... | âœ… stop/length/... | âœ… end_turn/max_tokens/... | âŒ |
| **System fingerprint** | âœ… | âŒ | âŒ | âŒ |
| **Native tokens** | âŒ | â­ SÃ­ | âŒ | âŒ |
| **Performance metrics** | âŒ | âŒ | âŒ | â­ SÃ­ (durations) |
| **Context array** | âŒ | âŒ | âŒ | â­ SÃ­ |
| **Streaming usage** | âœ… En Ãºltimo chunk | âŒ NULL | âœ… En evento message_delta | âœ… En Ãºltimo chunk |

---

## âŒ Datos que Estamos PERDIENDO Actualmente

### OpenAI
- `completion_tokens_details.reasoning_tokens`
- `completion_tokens_details.accepted_prediction_tokens`
- `completion_tokens_details.rejected_prediction_tokens`

### OpenRouter
- âŒ **CRÃTICO:** `usage` completo en streaming (viene NULL)
- `native_tokens_prompt`
- `native_tokens_completion`
- Rate limit headers

### Anthropic
- Eventos completos de streaming
- `stop_sequence` (cuÃ¡l fue)

### Ollama
- `context` array completo
- `load_duration`
- `prompt_eval_duration`
- `eval_duration`
- `total_duration`

---

## ðŸ’¡ Propuesta de SoluciÃ³n

### 1. **Agregar campo `raw_response` a tabla messages**
```sql
ALTER TABLE llm_manager_conversation_messages 
ADD COLUMN raw_response JSON NULL COMMENT 'Complete raw response from provider';
```

### 2. **Capturar respuesta completa en cada provider**
```php
// En cada Provider::stream()
return [
    'usage' => [...],
    'raw_response' => $lastResponse, // â­ NUEVO: Respuesta completa
];
```

### 3. **Guardar en DB**
```php
$assistantMessage = LLMConversationMessage::create([
    // ... campos actuales
    'raw_response' => $metrics['raw_response'] ?? null, // â­ NUEVO
    'metadata' => [
        // ... metadatos procesados actuales
    ],
]);
```

### 4. **Normalizar acceso a tokens**
Crear mÃ©todo en cada provider:
```php
public function extractTokenUsage($rawResponse): array
{
    // Cada provider implementa su lÃ³gica
    // OpenRouter: llamar endpoint generation si es null
    // Anthropic: parsear eventos
    // Ollama: leer eval_count
}
```

---

## ðŸŽ¯ Plan de ImplementaciÃ³n

**Fase 1: Captura (sin romper nada)**
1. âœ… Agregar migraciÃ³n `raw_response` column
2. âœ… Modificar cada provider para devolver `raw_response`
3. âœ… Guardar en DB sin procesar

**Fase 2: AnÃ¡lisis**
1. Recopilar datos reales de producciÃ³n
2. Analizar quÃ© campos Ãºnicos tiene cada provider
3. Documentar diferencias entre modelos del mismo provider

**Fase 3: OptimizaciÃ³n**
1. Implementar `extractTokenUsage()` inteligente
2. Agregar fallback para OpenRouter (HTTP call si usage=null)
3. Normalizar metadata keys

**Fase 4: Monitoring**
1. Dashboard de metadatos Ãºnicos
2. Alertas si faltan tokens
3. Comparativas de performance por provider

---

## ðŸš€ Siguiente Paso

**Â¿Empezamos con Fase 1?**
- Crear migraciÃ³n `raw_response`
- Modificar providers para capturar todo
- Guardar sin procesar (debugging completo)

Esto nos darÃ¡ visibilidad total de quÃ© estamos recibiendo realmente.
