# ðŸ”Œ LLM Streaming - Endpoints Explained

## Your Questions Answered

### â“ Pregunta 1: "Â¿CuÃ¡ntos endpoints hay para conectar con los modelos?"

**Respuesta: 5+ endpoints por proveedor**

No es un Ãºnico endpoint, sino mÃºltiples segÃºn la operaciÃ³n:

```
OLLAMA:
â”œâ”€â”€ /api/generate        â†’ Generar texto
â”œâ”€â”€ /api/embeddings      â†’ Generar embeddings
â”œâ”€â”€ /api/tags            â†’ Listar modelos disponibles
â”œâ”€â”€ /api/pull            â†’ Descargar modelo
â””â”€â”€ /api/show            â†’ Info del modelo

OPENAI:
â”œâ”€â”€ /v1/chat/completions     â†’ Chat streaming
â”œâ”€â”€ /v1/completions         â†’ Completions legacy
â”œâ”€â”€ /v1/embeddings          â†’ Embeddings
â””â”€â”€ /v1/models              â†’ Listar modelos

OPENROUTER (OpenAI-compatible):
â”œâ”€â”€ /api/v1/chat/completions â†’ Chat streaming
â””â”€â”€ (same as OpenAI)

ANTHROPIC:
â”œâ”€â”€ /v1/messages            â†’ Generar texto
â””â”€â”€ (diferente estructura)
```

---

### â“ Pregunta 2: "Â¿CÃ³mo sabe el sistema quÃ© endpoint usar?"

**Respuesta: El sistema estÃ¡ TOTALMENTE DINÃMICO - NO hay hardcoding**

#### Flow Completo:

```
1. USER selects model in dropdown
    â†’ selection: "OpenRoute" (ID=6)
    
2. BROWSER sends request with ?configuration_id=6
    
3. CONTROLLER receives request
    â†’ $configurationId = request('configuration_id')
    
4. DATABASE QUERY
    â†’ SELECT * FROM llm_manager_configurations WHERE id=6
    â†’ Gets:
        {
            provider: "openrouter",
            api_endpoint: "https://openrouter.ai/api/v1",
            api_key: "encrypted-key-123",
            model: "anthropic/claude-sonnet-4.5",
            default_parameters: {"temperature": 0.7, ...}
        }

5. MATCH PROVIDER TYPE
    â†’ if provider == "openrouter"
    â†’ return new OpenRouterProvider($configuration)
    
6. PROVIDER USES ENDPOINT
    â†’ OpenRouterProvider knows to append: /chat/completions
    â†’ Final URL: https://openrouter.ai/api/v1/chat/completions
    â†’ Sends request with api_key from config
    
7. STREAM RESPONSE
    â†’ Receives chunks from endpoint
    â†’ Sends to browser
```

---

## ðŸŽ¯ Key Points

### âœ… NO Hardcoding - Everything is Dynamic

| Component | Source | Example |
|-----------|--------|---------|
| Which provider to use | Database `provider` field | `"openrouter"` |
| Where to send request | Database `api_endpoint` field | `"https://openrouter.ai/api/v1"` |
| Authentication | Database `api_key` field (encrypted) | `encrypt("sk-or-...")` |
| Model name | Database `model` field | `"anthropic/claude-sonnet-4.5"` |
| Parameters | Database `default_parameters` JSON | `{"temperature": 0.7}` |

### âœ… Database-Driven Architecture

```php
// In Controller
$configuration = LLMConfiguration::findOrFail($configurationId);

// Now we have everything we need:
$configuration->provider         // â†’ Determines PHP class
$configuration->api_endpoint     // â†’ HTTP endpoint
$configuration->api_key          // â†’ Authentication
$configuration->model            // â†’ Model identifier
$configuration->default_parameters // â†’ Parameters
```

### âœ… Provider-Specific Logic

Each provider class knows how to use its endpoint:

```php
// OllamaProvider
$endpoint = rtrim($api_endpoint, '/') . '/api/generate';
// http://localhost:11434 + /api/generate

// OpenAIProvider  
$client = OpenAI::factory()
    ->withBaseUri($api_endpoint)  // https://api.openai.com/v1
    ->make();
// SDK appends /chat/completions

// OpenRouterProvider (same as OpenAI)
$client = OpenAI::factory()
    ->withBaseUri($api_endpoint)  // https://openrouter.ai/api/v1
    ->make();
// SDK appends /chat/completions
```

---

## ðŸ“Š Visual: How Parameters Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATABASE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ID=6 | OpenRoute | openrouter | https://openrouter.ai/api/v1   â”‚
â”‚      |           |            | {"temp": 0.7, "max_tokens": 4k} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Query by ID
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CONTROLLER                                  â”‚
â”‚  $config = LLMConfiguration::find(6)                            â”‚
â”‚  $provider = $manager->config(6)->getProvider()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Match provider="openrouter"
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROVIDER CLASS                                â”‚
â”‚  new OpenRouterProvider($config)                                â”‚
â”‚  â€¢ Receives $config with all parameters                         â”‚
â”‚  â€¢ Knows how to construct endpoint from $config->api_endpoint   â”‚
â”‚  â€¢ Extracts $config->api_key for auth                           â”‚
â”‚  â€¢ Uses $config->default_parameters as defaults                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Build request
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HTTP REQUEST                                  â”‚
â”‚  POST https://openrouter.ai/api/v1/chat/completions             â”‚
â”‚  Headers: Authorization: Bearer {$config->api_key}              â”‚
â”‚  Body: {                                                         â”‚
â”‚    "model": "anthropic/claude-sonnet-4.5",                       â”‚
â”‚    "temperature": 0.7,        â† from $config->default_parametersâ”‚
â”‚    "max_tokens": 4000,        â† from $config->default_parametersâ”‚
â”‚    "messages": [...]                                             â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Streaming response
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BROWSER (user)                                â”‚
â”‚  Real-time chunks arrive as SSE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” Why Ollama is Failing

### Your Ollama Configuration (Database):
```json
{
    "id": 1,
    "provider": "ollama",
    "api_endpoint": "http://localhost:11434",
    "model": "qwen3:4b"
}
```

### When you click "Send with Streaming":
```
1. Controller gets configuration_id=1
2. Loads from DB: provider="ollama", endpoint="http://localhost:11434"
3. Creates OllamaProvider with this config
4. OllamaProvider builds: http://localhost:11434 + /api/generate
5. Tries to connect: fopen("http://localhost:11434/api/generate")
6. âŒ FAILS because:
   - Ollama service NOT RUNNING
   - OR wrong IP/port
   - OR port 11434 blocked by firewall
```

### The Fix:
```bash
# 1. Start Ollama
/Applications/Ollama.app/Contents/MacOS/Ollama serve

# 2. Verify connection
curl http://localhost:11434/api/tags

# 3. If different machine/port, update database:
php artisan tinker
DB::table('llm_manager_configurations')
    ->where('id', 1)
    ->update(['api_endpoint' => 'http://NEW_IP:PORT']);
```

---

## ðŸŽ›ï¸ How to Add Multiple LLM Providers

### Add Local Ollama:
```php
[
    'provider' => 'ollama',
    'api_endpoint' => 'http://localhost:11434',
    'model' => 'qwen3:4b',
]
```

### Add Remote Ollama:
```php
[
    'provider' => 'ollama',
    'api_endpoint' => 'http://192.168.1.50:11434',  // â† Different
    'model' => 'mistral:7b',
]
```

### Add OpenAI:
```php
[
    'provider' => 'openai',
    'api_endpoint' => 'https://api.openai.com/v1',  // â† Different
    'api_key' => env('OPENAI_API_KEY'),
    'model' => 'gpt-4o',
]
```

### Add Custom LLM Server:
```php
[
    'provider' => 'custom',
    'api_endpoint' => 'http://my-llm.com/api/generate',  // â† Your URL
    'api_key' => 'custom-token',
    'model' => 'my-model',
]
```

---

## ðŸ“‹ Architecture Summary

### Everything is Configured in Database âœ…
- No hardcoded endpoints
- No environment variables needed
- Just add a row to `llm_manager_configurations`

### Dynamic Routing âœ…
- User selects model â†’ Gets configuration from DB
- Configuration determines provider class
- Provider builds endpoint from configuration
- Request sent to configured endpoint

### Provider-Agnostic âœ…
- Same flow works for Ollama, OpenAI, OpenRouter, etc.
- Each provider knows its endpoint structure
- System doesn't care about provider specifics

### Fully Extensible âœ…
- Add new provider? Create new PHP class
- Add new endpoint? Update database row
- Add new server? Create new configuration

---

## ðŸš€ Testing Flow

```bash
# 1. Check database config
php artisan tinker << 'EOF'
$config = DB::table('llm_manager_configurations')->find(1);
echo "Endpoint: {$config->api_endpoint}\n";
EOF

# 2. Test curl
curl http://localhost:11434/api/tags

# 3. Try web UI
# Select model â†’ Send message â†’ Check if streaming works

# 4. If fails, debug
# Check Ollama: lsof -i :11434
# Check network: ping localhost
# Check models: ollama list
```

---

## ðŸ’¡ Key Takeaways

| Question | Answer |
|----------|--------|
| Endpoints hardcoded? | **NO** - all from database |
| How does it know which endpoint? | **Reads provider field â†’ selects class** |
| Can I use multiple servers? | **YES** - create multiple configs |
| Can I switch providers dynamically? | **YES** - dropdown selector in UI |
| What if Ollama is down? | **Switch to OpenRouter/OpenAI** (configured) |
| Can I use custom LLM server? | **YES** - "custom" provider type |

---

### Next Steps:
1. Start Ollama: `/Applications/Ollama.app/Contents/MacOS/Ollama serve`
2. Verify: `curl http://localhost:11434/api/tags`
3. Pull models: `ollama pull qwen3:4b`
4. Try web UI: Select "Ollama Qwen 3" â†’ Send message
5. Should work! âœ…

If still fails, check the detailed debugging guide: `OLLAMA-DEBUG-GUIDE.md`
